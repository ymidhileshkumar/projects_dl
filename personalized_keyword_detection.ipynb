{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg+torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Rest of the code remains unchanged...\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    label_map = {\"baankud'qaaa\": 0, \"bad'qa_aangura\": 1, \"anJa_chot'a_kalaa\": 2, \"baaluraghaat'a\": 3, \"baandhaakapi\": 4, \"anJa_bad'qa_kalaa\": 5, \"anJaanJa_haansa\": 6, \"anJaanJa_tailabiija\": 7, \"aapela\": 8, \"ad'qahara_d'aala\": 9,\"anJaanJa_aalu\": 10, \"anJaanJa_d'ima\": 11, \"aalu\": 12, \"aama\": 13, \"aanaarasa\": 14, \"aaii_aara_t'oyent'i\": 15, \"aalipuraduyaara\": 16, \"aalura_sanna_caala\": 17, \"aaii_aara_chatrisha\": 18, \"aaii_aara_kud'qi\": 19}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_dataset'\n",
        "    label_map = {\"baankud'qaaa\": 0, \"bad'qa_aangura\": 1, \"anJa_chot'a_kalaa\": 2, \"baaluraghaat'a\": 3, \"baandhaakapi\": 4, \"anJa_bad'qa_kalaa\": 5, \"anJaanJa_haansa\": 6, \"anJaanJa_tailabiija\": 7, \"aapela\": 8, \"ad'qahara_d'aala\": 9,\"anJaanJa_aalu\": 10, \"anJaanJa_d'ima\": 11, \"aalu\": 12, \"aama\": 13, \"aanaarasa\": 14, \"aaii_aara_t'oyent'i\": 15, \"aalipuraduyaara\": 16, \"aalura_sanna_caala\": 17, \"aaii_aara_chatrisha\": 18, \"aaii_aara_kud'qi\": 19}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/keyword_spotting_model_6.pth')\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_dataset'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_spotting_model_6.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RWQSEwsfo3Gs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "9f2f10d8-3fa7-4653-824d-4429b1bd4dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-efce931e6588>\u001b[0m in \u001b[0;36m<cell line: 206>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-efce931e6588>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTCResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchannel_scale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1149\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h3FEZsAhwDAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hc8nhYqS_5T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torchvision.transforms import Compose\n",
        "import os\n",
        "import torch.nn as nn  # Import torch.nn module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import librosa\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "# Load the saved model\n",
        "bins = 40\n",
        "channels = [64, 128, 256, 512]\n",
        "channel_scale = 1\n",
        "num_classes = 20\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/keyword_spotting_model_6.pth'\n",
        "model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Define the transform to extract features\n",
        "# transform = Compose([\n",
        "#     torchaudio.transforms.MFCC(\n",
        "#         sample_rate=16000,  # Sample rate of the audio file\n",
        "#         n_mfcc=40,          # Number of MFCC coefficients to extract\n",
        "#     )\n",
        "# ])\n",
        "\n",
        "# Function to predict the class of an audio file\n",
        "# def predict_class(file_path, model, transform):\n",
        "#     waveform, sample_rate = torchaudio.load(file_path)\n",
        "#     if waveform.size(0) == 1:\n",
        "#       print(\"The audio is already mono\")\n",
        "#     else:\n",
        "#       waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "#     # Take the average of the two channels to convert to mono\n",
        "\n",
        "#     features = transform(waveform).unsqueeze(0)  # Add batch dimension\n",
        "#     outputs = model(features)\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.data)\n",
        "#     print(_)\n",
        "#     print(predicted)\n",
        "#     return predicted.item()\n",
        "\n",
        "def pad_or_trim_mfcc(mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length >fixed_length:\n",
        "            mfcc = mfcc[:, :, :fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = \"/content/drive/MyDrive/testing_dataset/aaii_aara_chatrisha/FBNWB19B0035I031.wav\"\n",
        "waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "# If the waveform is stereo, convert it to mono\n",
        "if waveform.size(0) == 2:\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "\n",
        "  # Load as mono channel\n",
        "\n",
        "      # Resample the audio if the sample rate is different from the target\n",
        "\n",
        "\n",
        "      # Normalize the amplitude to [-1, 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mfcc_window_size=0.025\n",
        "mfcc_window_stride=0.01\n",
        "mfcc_n_mels=40\n",
        "fixed_length=500\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "mfcc = pad_or_trim_mfcc(mfcc)\n",
        "mfcc = mfcc.permute(0, 2, 1)\n",
        "outputs = model(mfcc.unsqueeze(0))\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "label_map = {\"baankud'qaaa\": 0, \"bad'qa_aangura\": 1, \"anJa_chot'a_kalaa\": 2, \"baaluraghaat'a\": 3, \"baandhaakapi\": 4, \"anJa_bad'qa_kalaa\": 5, \"anJaanJa_haansa\": 6, \"anJaanJa_tailabiija\": 7, \"aapela\": 8, \"ad'qahara_d'aala\": 9,\"anJaanJa_aalu\": 10, \"anJaanJa_d'ima\": 11, \"aalu\": 12, \"aama\": 13, \"aanaarasa\": 14, \"aaii_aara_t'oyent'i\": 15, \"aalipuraduyaara\": 16, \"aalura_sanna_caala\": 17, \"aaii_aara_chatrisha\": 18, \"aaii_aara_kud'qi\": 19}\n",
        "print(\"Predicted class index:\",list(label_map.items())[predicted.item()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJw-4LwqwMGv",
        "outputId": "c10fcdf9-5dfe-4969-c7f7-2fdf4f29463d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: ('aalura_sanna_caala', 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate = torchaudio.load(\"/content/drive/MyDrive/training_dataset/ad'qahara_d'aala/FBNWB06B0065I047.wav\")\n",
        "sample_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtAJ33Pxzpe-",
        "outputId": "b731e214-4f45-48bf-a8fb-efe26204e2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kLOZPOEzNAF",
        "outputId": "9f7239c4-ca61-4d1a-90b3-8e85ec1e0458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 24000])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(waveform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyqwpyTK5Q6c",
        "outputId": "5421f2f7-2054-4025-be43-379e7e49dc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate = librosa.load(\"/content/drive/MyDrive/training_dataset/ad'qahara_d'aala/FBNWB06B0065I047.wav\", sr=None, mono=True)"
      ],
      "metadata": {
        "id": "T-CnBpGl5Exl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c=torch.tensor(waveform)"
      ],
      "metadata": {
        "id": "Snxjuaei5Jv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00r9SlGy5plW",
        "outputId": "8ccd0b84-0537-4505-8c17-a88f65b29533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([24000])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate = torchaudio.load(\"/content/drive/MyDrive/training_dataset/ad'qahara_d'aala/FBNWB06B0065I047.wav\")\n",
        "sample_rate"
      ],
      "metadata": {
        "id": "4VSBnKNq47gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torchvision.transforms import Compose\n",
        "import os\n",
        "import torch.nn as nn  # Import torch.nn module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import librosa\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "# Load the saved model\n",
        "bins = 40\n",
        "channels = [64, 128, 256, 512]\n",
        "channel_scale = 1\n",
        "num_classes = 20\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/keyword_spotting_model_6.pth'\n",
        "model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Define the transform to extract features\n",
        "# transform = Compose([\n",
        "#     torchaudio.transforms.MFCC(\n",
        "#         sample_rate=16000,  # Sample rate of the audio file\n",
        "#         n_mfcc=40,          # Number of MFCC coefficients to extract\n",
        "#     )\n",
        "# ])\n",
        "\n",
        "# Function to predict the class of an audio file\n",
        "# def predict_class(file_path, model, transform):\n",
        "#     waveform, sample_rate = torchaudio.load(file_path)\n",
        "#     if waveform.size(0) == 1:\n",
        "#       print(\"The audio is already mono\")\n",
        "#     else:\n",
        "#       waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "#     # Take the average of the two channels to convert to mono\n",
        "\n",
        "#     features = transform(waveform).unsqueeze(0)  # Add batch dimension\n",
        "#     outputs = model(features)\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.data)\n",
        "#     print(_)\n",
        "#     print(predicted)\n",
        "#     return predicted.item()\n",
        "\n",
        "def pad_or_trim_mfcc(mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length >fixed_length:\n",
        "            mfcc = mfcc[:, :, :fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = \"/content/drive/MyDrive/bankura.wav\"\n",
        "waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "if waveform.size(0)!=1:\n",
        "  waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "\n",
        "\n",
        "target_sample_rate = 8000\n",
        "if sample_rate!=target_sample_rate:\n",
        "  target_sample_rate = 8000\n",
        "  resampler = Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
        "  waveform = resampler(waveform)\n",
        "  sample_rate=target_sample_rate\n",
        "\n",
        "max_amplitude = torch.max(waveform)\n",
        "min_amplitude =torch.max(waveform)\n",
        "# Normalize the waveform between -1 and 1\n",
        "if max_amplitude<=1 and min_amplitude>=-1  :\n",
        "    max_amplitude1 = torch.max(torch.abs(waveform))\n",
        "    waveform = waveform / max_amplitude1\n",
        "\n",
        "mfcc_window_size=0.025\n",
        "mfcc_window_stride=0.01\n",
        "mfcc_n_mels=40\n",
        "fixed_length=500\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "mfcc = pad_or_trim_mfcc(mfcc)\n",
        "mfcc = mfcc.permute(0, 2, 1)\n",
        "outputs = model(mfcc.unsqueeze(0))\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "label_map = {\"baankud'qaaa\": 0, \"bad'qa_aangura\": 1, \"anJa_chot'a_kalaa\": 2, \"baaluraghaat'a\": 3, \"baandhaakapi\": 4, \"anJa_bad'qa_kalaa\": 5, \"anJaanJa_haansa\": 6, \"anJaanJa_tailabiija\": 7, \"aapela\": 8, \"ad'qahara_d'aala\": 9,\"anJaanJa_aalu\": 10, \"anJaanJa_d'ima\": 11, \"aalu\": 12, \"aama\": 13, \"aanaarasa\": 14, \"aaii_aara_t'oyent'i\": 15, \"aalipuraduyaara\": 16, \"aalura_sanna_caala\": 17, \"aaii_aara_chatrisha\": 18, \"aaii_aara_kud'qi\": 19}\n",
        "print(\"Predicted class index:\",list(label_map.items())[predicted.item()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G1fDr4E95o8",
        "outputId": "2e9845bd-7a71-442c-c175-47af17a12b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: (\"ad'qahara_d'aala\", 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOc3uAGc-o4z",
        "outputId": "0bf54ae9-c9b7-4a12-b81c-14418874a854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"baankud'qaaa\": 0, \"bad'qa_aangura\": 1, \"anJa_chot'a_kalaa\": 2, \"baaluraghaat'a\": 3, \"baandhaakapi\": 4, \"anJa_bad'qa_kalaa\": 5, \"anJaanJa_haansa\": 6, \"anJaanJa_tailabiija\": 7, \"aapela\": 8, \"ad'qahara_d'aala\": 9,\"anJaanJa_aalu\": 10, \"anJaanJa_d'ima\": 11, \"aalu\": 12, \"aama\": 13, \"aanaarasa\": 14, \"aaii_aara_t'oyent'i\": 15, \"aalipuraduyaara\": 16, \"aalura_sanna_caala\": 17, \"aaii_aara_chatrisha\": 18, \"aaii_aara_kud'qi\": 19}"
      ],
      "metadata": {
        "id": "0ZA-JhUxAp2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "sou_dir='/content/drive/MyDrive/testing_dataset'\n",
        "class_names=os.listdir(sou_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "print(label_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU4xGsvvRHm6",
        "outputId": "a1fda36f-dc29-4f7a-c7c5-8acb82fab4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"bad'qa_aangura\": 0, \"baankud'qaaa\": 1, \"anJa_bad'qa_kalaa\": 2, 'anJaanJa_tailabiija': 3, 'anJaanJa_haansa': 4, \"baaluraghaat'a\": 5, 'baandhaakapi': 6, \"anJaanJa_d'ima\": 7, \"anJa_chot'a_kalaa\": 8, \"ad'qahara_d'aala\": 9, 'anJaanJa_aalu': 10, 'aapela': 11, \"aaii_aara_t'oyent'i\": 12, 'aalu': 13, 'aalipuraduyaara': 14, \"aaii_aara_kud'qi\": 15, 'aalura_sanna_caala': 16, 'aama': 17, 'aaii_aara_chatrisha': 18, 'aanaarasa': 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchvision.transforms import Compose\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define your TCResNet model and load the pretrained weights\n",
        "bins = 40\n",
        "channels = [64, 128, 256, 512]\n",
        "channel_scale = 1\n",
        "num_classes = 20\n",
        "batch_size=32\n",
        "fixed_length=500\n",
        "label_map = {\"baankud'qaaa\": 0, \"bad'qa_aangura\": 1, \"anJa_chot'a_kalaa\": 2, \"baaluraghaat'a\": 3, \"baandhaakapi\": 4, \"anJa_bad'qa_kalaa\": 5, \"anJaanJa_haansa\": 6, \"anJaanJa_tailabiija\": 7, \"aapela\": 8, \"ad'qahara_d'aala\": 9,\"anJaanJa_aalu\": 10, \"anJaanJa_d'ima\": 11, \"aalu\": 12, \"aama\": 13, \"aanaarasa\": 14, \"aaii_aara_t'oyent'i\": 15, \"aalipuraduyaara\": 16, \"aalura_sanna_caala\": 17, \"aaii_aara_chatrisha\": 18, \"aaii_aara_kud'qi\": 19}\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    # Define your model architecture here...\n",
        "\n",
        "# Load the saved model\n",
        "model_path ='/content/drive/MyDrive/keyword_spotting_model_6.pth'\n",
        "model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Define the transform to extract features\n",
        "transform = Compose([\n",
        "    torchaudio.transforms.MFCC(\n",
        "        sample_rate=16000,  # Sample rate of the audio file\n",
        "        n_mfcc=40,          # Number of MFCC coefficients to extract\n",
        "    )\n",
        "])\n",
        "\n",
        "# Define your validation dataset\n",
        "validation_data_dir ='/content/drive/MyDrive/testing_dataset'\n",
        "validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Function to predict the class of an audio file\n",
        "def predict_class(file_path, model, transform):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    if waveform.size(0) == 1:\n",
        "      print(\"The audio is already mono\")\n",
        "    else:\n",
        "      waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Take the average of the two channels to convert to mono\n",
        "\n",
        "    features = transform(waveform).unsqueeze(0)  # Add batch dimension\n",
        "    outputs = model(features)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Create lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "# Iterate over the validation dataset\n",
        "# Iterate over the validation dataset\n",
        "for inputs, labels in validation_dataloader:\n",
        "    inputs = inputs.squeeze(1)  # Remove the channel dimension (batch_size, 1, bins, frames) -> (batch_size, bins, frames)\n",
        "    inputs = inputs.to(device)  # Move inputs to device\n",
        "    labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "    # Make sure input has only 1 channel\n",
        "    inputs = inputs.unsqueeze(1) # Select only the first channel\n",
        "\n",
        "    # Make predictions\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Append true and predicted labels to lists\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "    predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4luvuEt0__Q",
        "outputId": "379d8719-9bed-44b6-8cbd-7d93c7b73df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[19  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 16  0  0  0  0  0  0  0  1  0  1  1  0  0  0  1  0  0  0]\n",
            " [ 0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  2 17  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 16  0  1  0  0  0  0  0  1  0  0  2  0  0  0  0]\n",
            " [ 0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 18  0  1  0  1  0  0  0  0  0  0]\n",
            " [ 0  1  0  0  0  0  0  0  0  1 17  0  0  0  0  0  0  1  0  0]\n",
            " [ 0  1  0  0  1  0  0  2  0  0  0 15  0  0  0  0  1  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  0  1  1  0  0 16  0  0  1  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  1  0  1 17  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0 19  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18  0  0  1  1]\n",
            " [ 1  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0]\n",
            " [ 0  0  0  1  0  0  0  0  0  0  0  0  2  0  0  0  0 17  0  0]\n",
            " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0 18  0]\n",
            " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the TCResNet model\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names=os.listdir(data_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg+torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Rest of the code remains unchanged...\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "    class_names=os.listdir(data_dir)\n",
        "    label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/speaker_spotting_model.pth')\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_Speakers_dataset'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset\n",
        "    test_saved_model('/content/drive/MyDrive/speaker_spotting_model.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbZUx0pON_9l",
        "outputId": "f6f61d64-7adb-4553-860a-ff2954098d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0118': 0, '0142': 1, '0149': 2, '0122': 3, '0143': 4, '0147': 5, '0127': 6, '0146': 7, '0135': 8, '0108': 9, '0073': 10, '0106': 11, '0100': 12, '0014': 13, '0009': 14, '0102': 15, '0099': 16, '0041': 17, '0084': 18, '0006': 19}\n",
            "Epoch [1/50], Loss: 2.9495139932632446\n",
            "Epoch [2/50], Loss: 2.7924093627929687\n",
            "Epoch [3/50], Loss: 2.6539785432815552\n",
            "Epoch [4/50], Loss: 2.527123007774353\n",
            "Epoch [5/50], Loss: 2.370954647064209\n",
            "Epoch [6/50], Loss: 2.271669263839722\n",
            "Epoch [7/50], Loss: 2.1229736375808717\n",
            "Epoch [8/50], Loss: 2.0101923084259035\n",
            "Epoch [9/50], Loss: 1.8908518409729005\n",
            "Epoch [10/50], Loss: 1.7904393458366394\n",
            "Epoch [11/50], Loss: 1.618246672153473\n",
            "Epoch [12/50], Loss: 1.4680297923088075\n",
            "Epoch [13/50], Loss: 1.320442681312561\n",
            "Epoch [14/50], Loss: 1.194735586643219\n",
            "Epoch [15/50], Loss: 1.1210867166519165\n",
            "Epoch [16/50], Loss: 0.9539926409721374\n",
            "Epoch [17/50], Loss: 0.8170857137441635\n",
            "Epoch [18/50], Loss: 0.7231274038553238\n",
            "Epoch [19/50], Loss: 0.5983017748594284\n",
            "Epoch [20/50], Loss: 0.4860734388232231\n",
            "Epoch [21/50], Loss: 0.42539854407310485\n",
            "Epoch [22/50], Loss: 0.3161561819911003\n",
            "Epoch [23/50], Loss: 0.2775924426317215\n",
            "Epoch [24/50], Loss: 0.23934102892875672\n",
            "Epoch [25/50], Loss: 0.15746699184179305\n",
            "Epoch [26/50], Loss: 0.11243901960551739\n",
            "Epoch [27/50], Loss: 0.06463075842708349\n",
            "Epoch [28/50], Loss: 0.03897902812808752\n",
            "Epoch [29/50], Loss: 0.025141068380326034\n",
            "Epoch [30/50], Loss: 0.02225900862365961\n",
            "Epoch [31/50], Loss: 0.018394415900111198\n",
            "Epoch [32/50], Loss: 0.015312229339033366\n",
            "Epoch [33/50], Loss: 0.012406188822351396\n",
            "Epoch [34/50], Loss: 0.008129575913771987\n",
            "Epoch [35/50], Loss: 0.006995051638223231\n",
            "Epoch [36/50], Loss: 0.005814961106516421\n",
            "Epoch [37/50], Loss: 0.005487407795153558\n",
            "Epoch [38/50], Loss: 0.005235326122492552\n",
            "Epoch [39/50], Loss: 0.004348044595681131\n",
            "Epoch [40/50], Loss: 0.004371806932613254\n",
            "Epoch [41/50], Loss: 0.003870985368266702\n",
            "Epoch [42/50], Loss: 0.0036234019370749593\n",
            "Epoch [43/50], Loss: 0.00345373566262424\n",
            "Epoch [44/50], Loss: 0.003014279536437243\n",
            "Epoch [45/50], Loss: 0.002736136286985129\n",
            "Epoch [46/50], Loss: 0.0026819168333895504\n",
            "Epoch [47/50], Loss: 0.0024409153242595494\n",
            "Epoch [48/50], Loss: 0.0022565128724090756\n",
            "Epoch [49/50], Loss: 0.0020160628436133267\n",
            "Epoch [50/50], Loss: 0.002249863762408495\n",
            "Validation Accuracy of the saved model: 71.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names=os.listdir(data_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "# Define the dataset class\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {class_name: idx for idx, class_name in enumerate(label_map)}\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg + torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "    class_names = os.listdir(data_dir)\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Load the saved model\n",
        "    model_path = '/content/drive/MyDrive/speaker_spotting_model.pth'\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model for additional epochs\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the updated model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/speaker_spotting_model_updated.pth')\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_Speakers_dataset'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_saved_model('/content/drive/MyDrive/speaker_spotting_model_updated.pth', validation_dataloader, device)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeXhcLDqeQL_",
        "outputId": "23095cd1-3217-4f9a-ff14-3ba91ed011fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1467542373528704\n",
            "Epoch [2/10], Loss: 0.10414267241954804\n",
            "Epoch [3/10], Loss: 0.08488971516489982\n",
            "Epoch [4/10], Loss: 0.06052946258336306\n",
            "Epoch [5/10], Loss: 0.14050185441970825\n",
            "Epoch [6/10], Loss: 0.1127864619717002\n",
            "Epoch [7/10], Loss: 0.10904358629137277\n",
            "Epoch [8/10], Loss: 0.08482573207467795\n",
            "Epoch [9/10], Loss: 0.06355442682281137\n",
            "Epoch [10/10], Loss: 0.04759636629372835\n",
            "Validation Accuracy of the saved model: 62.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names=os.listdir(data_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "# Define the dataset class\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {class_name: idx for idx, class_name in enumerate(label_map)}\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg + torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "    class_names = os.listdir(data_dir)\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Load the saved model\n",
        "    model_path = '/content/drive/MyDrive/speaker_spotting_model_updated.pth'\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model for additional epochs\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the updated model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/speaker_spotting_model_updated_1.pth')\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_Speakers_dataset'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_saved_model('/content/drive/MyDrive/speaker_spotting_model_updated_1.pth', validation_dataloader, device)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaGQalHUg7sm",
        "outputId": "9f603d23-5e19-48cd-c80a-e80a90a05726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1173825503513217\n",
            "Epoch [2/10], Loss: 0.10740462448447943\n",
            "Epoch [3/10], Loss: 0.10052462661638856\n",
            "Epoch [4/10], Loss: 0.030875644106417896\n",
            "Epoch [5/10], Loss: 0.009279043045826257\n",
            "Epoch [6/10], Loss: 0.007782989935949444\n",
            "Epoch [7/10], Loss: 0.005618911295896396\n",
            "Epoch [8/10], Loss: 0.0024156421527732163\n",
            "Epoch [9/10], Loss: 0.001341382401296869\n",
            "Epoch [10/10], Loss: 0.0017499962920555846\n",
            "Validation Accuracy of the saved model: 67.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names=os.listdir(data_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "# Define the dataset class\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {class_name: idx for idx, class_name in enumerate(label_map)}\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg + torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "    class_names = os.listdir(data_dir)\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Load the saved model\n",
        "    model_path = '/content/drive/MyDrive/speaker_spotting_model_updated_1.pth'\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model for additional epochs\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the updated model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/speaker_spotting_model_updated_2.pth')\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_Speakers_dataset'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_saved_model('/content/drive/MyDrive/speaker_spotting_model_updated_2.pth', validation_dataloader, device)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rngk9CwiV5W",
        "outputId": "661c8f00-a9ea-4866-ed4f-678392d1a570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.06728889818885364\n",
            "Epoch [2/10], Loss: 0.11582761101424693\n",
            "Epoch [3/10], Loss: 0.10087040040642023\n",
            "Epoch [4/10], Loss: 0.07732015829533338\n",
            "Epoch [5/10], Loss: 0.03970345253124833\n",
            "Epoch [6/10], Loss: 0.023391819056123496\n",
            "Epoch [7/10], Loss: 0.014391174898482859\n",
            "Epoch [8/10], Loss: 0.01320849381852895\n",
            "Epoch [9/10], Loss: 0.024677844303660094\n",
            "Epoch [10/10], Loss: 0.07869688289240002\n",
            "Validation Accuracy of the saved model: 58.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the TCResNet model\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names=os.listdir(data_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg+torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Rest of the code remains unchanged...\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "    class_names=os.listdir(data_dir)\n",
        "    label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 70\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/speaker_spotting_model_1.pth')\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_Speakers_dataset'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset\n",
        "    test_saved_model('/content/drive/MyDrive/speaker_spotting_model_1.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUoocF7ij1GJ",
        "outputId": "c78b2dbf-01d8-47e0-b792-745c46fd4ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0118': 0, '0142': 1, '0149': 2, '0122': 3, '0143': 4, '0147': 5, '0127': 6, '0146': 7, '0135': 8, '0108': 9, '0073': 10, '0106': 11, '0100': 12, '0014': 13, '0009': 14, '0102': 15, '0099': 16, '0041': 17, '0084': 18, '0006': 19}\n",
            "Epoch [1/70], Loss: 2.9709414529800413\n",
            "Epoch [2/70], Loss: 2.8555618810653685\n",
            "Epoch [3/70], Loss: 2.726030430793762\n",
            "Epoch [4/70], Loss: 2.5959487438201903\n",
            "Epoch [5/70], Loss: 2.4297839307785036\n",
            "Epoch [6/70], Loss: 2.341527009010315\n",
            "Epoch [7/70], Loss: 2.213525950908661\n",
            "Epoch [8/70], Loss: 2.111702995300293\n",
            "Epoch [9/70], Loss: 1.963126814365387\n",
            "Epoch [10/70], Loss: 1.864241771697998\n",
            "Epoch [11/70], Loss: 1.732000651359558\n",
            "Epoch [12/70], Loss: 1.5751424956321716\n",
            "Epoch [13/70], Loss: 1.46021320104599\n",
            "Epoch [14/70], Loss: 1.39758819937706\n",
            "Epoch [15/70], Loss: 1.1876923048496246\n",
            "Epoch [16/70], Loss: 1.0353247392177582\n",
            "Epoch [17/70], Loss: 0.9225025129318237\n",
            "Epoch [18/70], Loss: 0.8088839465379715\n",
            "Epoch [19/70], Loss: 0.7049153435230255\n",
            "Epoch [20/70], Loss: 0.5860136795043945\n",
            "Epoch [21/70], Loss: 0.4797085124254227\n",
            "Epoch [22/70], Loss: 0.37888716340065004\n",
            "Epoch [23/70], Loss: 0.30797184973955155\n",
            "Epoch [24/70], Loss: 0.24546321377158165\n",
            "Epoch [25/70], Loss: 0.17840675622224808\n",
            "Epoch [26/70], Loss: 0.17599779412150382\n",
            "Epoch [27/70], Loss: 0.12638926960527896\n",
            "Epoch [28/70], Loss: 0.12008541278541088\n",
            "Epoch [29/70], Loss: 0.08869406722486019\n",
            "Epoch [30/70], Loss: 0.05212323419749737\n",
            "Epoch [31/70], Loss: 0.030679063759744166\n",
            "Epoch [32/70], Loss: 0.018426744975149633\n",
            "Epoch [33/70], Loss: 0.01260339504107833\n",
            "Epoch [34/70], Loss: 0.009078968781977892\n",
            "Epoch [35/70], Loss: 0.007956566605716944\n",
            "Epoch [36/70], Loss: 0.006975557208061218\n",
            "Epoch [37/70], Loss: 0.006171078970655799\n",
            "Epoch [38/70], Loss: 0.005248996298760176\n",
            "Epoch [39/70], Loss: 0.004711565384641289\n",
            "Epoch [40/70], Loss: 0.005117227830924094\n",
            "Epoch [41/70], Loss: 0.00428056207485497\n",
            "Epoch [42/70], Loss: 0.0037115072575397787\n",
            "Epoch [43/70], Loss: 0.003670414513908327\n",
            "Epoch [44/70], Loss: 0.003427647450007498\n",
            "Epoch [45/70], Loss: 0.003397533567622304\n",
            "Epoch [46/70], Loss: 0.003016671675723046\n",
            "Epoch [47/70], Loss: 0.0027607430913485585\n",
            "Epoch [48/70], Loss: 0.002339067067950964\n",
            "Epoch [49/70], Loss: 0.0023351466073654593\n",
            "Epoch [50/70], Loss: 0.002333426357945427\n",
            "Epoch [51/70], Loss: 0.0019545296858996153\n",
            "Epoch [52/70], Loss: 0.002230893124360591\n",
            "Epoch [53/70], Loss: 0.0018410401977598668\n",
            "Epoch [54/70], Loss: 0.006425759568810463\n",
            "Epoch [55/70], Loss: 0.5561801412142813\n",
            "Epoch [56/70], Loss: 1.0340075600147247\n",
            "Epoch [57/70], Loss: 0.4265859042108059\n",
            "Epoch [58/70], Loss: 0.17116110965609552\n",
            "Epoch [59/70], Loss: 0.05602999877184629\n",
            "Epoch [60/70], Loss: 0.04870390301570296\n",
            "Epoch [61/70], Loss: 0.017547724517062307\n",
            "Epoch [62/70], Loss: 0.009285893570631743\n",
            "Epoch [63/70], Loss: 0.0064600498881191015\n",
            "Epoch [64/70], Loss: 0.005484651220031083\n",
            "Epoch [65/70], Loss: 0.0046004768460989\n",
            "Epoch [66/70], Loss: 0.004254090785980225\n",
            "Epoch [67/70], Loss: 0.003678372229915112\n",
            "Epoch [68/70], Loss: 0.0031823299615643917\n",
            "Epoch [69/70], Loss: 0.0032540932600386443\n",
            "Epoch [70/70], Loss: 0.0028814761829562487\n",
            "Validation Accuracy of the saved model: 67.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torchvision.transforms import Compose\n",
        "import os\n",
        "import torch.nn as nn  # Import torch.nn module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Load the saved model\n",
        "bins = 40\n",
        "channels = [64, 128, 256, 512]\n",
        "channel_scale = 1\n",
        "num_classes = 20\n",
        "data_dir = '/content/drive/MyDrive/training_dataset'\n",
        "class_names=os.listdir(data_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "data_dir1 = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names1=os.listdir(data_dir1)\n",
        "label_map1={class_name: idx for idx,class_name in enumerate(class_names1)}\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/keyword_spotting_model_6.pth'\n",
        "model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "model_path1 = '/content/drive/MyDrive/speaker_spotting_model.pth'\n",
        "model1 = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model1.load_state_dict(torch.load(model_path1, map_location=torch.device('cpu')))\n",
        "model1.eval()\n",
        "\n",
        "# Define the transform to extract features\n",
        "transform = Compose([\n",
        "    torchaudio.transforms.MFCC(\n",
        "        sample_rate=16000,  # Sample rate of the audio file\n",
        "        n_mfcc=40,          # Number of MFCC coefficients to extract\n",
        "    )\n",
        "])\n",
        "\n",
        "# Function to predict the class of an audio file\n",
        "def predict_class(file_path, model, transform):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    if waveform.size(0) == 1:\n",
        "      print(\"The audio is already mono\")\n",
        "    else:\n",
        "      waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Take the average of the two channels to convert to mono\n",
        "\n",
        "    features = transform(waveform).unsqueeze(0)  # Add batch dimension\n",
        "    outputs = model(features)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    print(outputs)\n",
        "    print(outputs.data)\n",
        "    print(_)\n",
        "    print(predicted)\n",
        "    return predicted.item()\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = '/content/drive/MyDrive/training_Speakers_dataset/0009/FBNWB01B0009I036.wav'\n",
        "predicted_class_index = predict_class(audio_file_path, model, transform)\n",
        "predicted_speaker_index = predict_class(audio_file_path, model1, transform)\n",
        "print(\"Predicted class index:\", list(label_map.items())[predicted_class_index])\n",
        "print(\"Predicted speaker index:\", list(label_map1.items())[predicted_speaker_index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFkvP8USm3EY",
        "outputId": "f02bce1c-6d21-45cc-9e0a-a5b8cf2b817b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The audio is already mono\n",
            "tensor([[ -79.2868,  128.0415, -152.2669,  -72.4877,  -18.9672,  -46.3931,\n",
            "         -149.1107,  -49.5355,  -39.7881,  -85.2858,   64.6844,  -28.6762,\n",
            "          153.4448,  -86.6532, -128.2800,  -98.2132,  -29.3943,  -29.9324,\n",
            "         -137.2670,  -32.1713]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -79.2868,  128.0415, -152.2669,  -72.4877,  -18.9672,  -46.3931,\n",
            "         -149.1107,  -49.5355,  -39.7881,  -85.2858,   64.6844,  -28.6762,\n",
            "          153.4448,  -86.6532, -128.2800,  -98.2132,  -29.3943,  -29.9324,\n",
            "         -137.2670,  -32.1713]])\n",
            "tensor([153.4448])\n",
            "tensor([12])\n",
            "The audio is already mono\n",
            "tensor([[-12.9864,   4.8964,   0.1180, -26.3453,  -2.3622, -11.7190, -27.7962,\n",
            "          23.4866, -15.6716,  -4.3217, -10.6815, -12.3820,  -1.8832, -30.7854,\n",
            "         -33.4417,  -2.7797,  -2.8027, -11.6420, -21.8771,   6.1512]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-12.9864,   4.8964,   0.1180, -26.3453,  -2.3622, -11.7190, -27.7962,\n",
            "          23.4866, -15.6716,  -4.3217, -10.6815, -12.3820,  -1.8832, -30.7854,\n",
            "         -33.4417,  -2.7797,  -2.8027, -11.6420, -21.8771,   6.1512]])\n",
            "tensor([23.4866])\n",
            "tensor([7])\n",
            "Predicted class index: (\"aaii_aara_kud'qi\", 12)\n",
            "Predicted speaker index: ('0146', 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torchvision.transforms import Compose\n",
        "import os\n",
        "import torch.nn as nn  # Import torch.nn module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import librosa\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "# Load the saved model\n",
        "bins = 40\n",
        "channels = [64, 128, 256, 512]\n",
        "channel_scale = 1\n",
        "num_classes = 20\n",
        "\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/keyword_spotting_model_6.pth'\n",
        "model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "model_path1 = '/content/drive/MyDrive/speaker_spotting_model.pth'\n",
        "model1 = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model1.load_state_dict(torch.load(model_path1, map_location=torch.device('cpu')))\n",
        "model1.eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pad_or_trim_mfcc(mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length >fixed_length:\n",
        "            mfcc = mfcc[:, :, :fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = \"/content/drive/MyDrive/testing_Speakers_dataset/0041/FBNWB03B0041I040.wav\"\n",
        "waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "if waveform.size(0)!=1:\n",
        "  waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "\n",
        "\n",
        "target_sample_rate = 16000\n",
        "if sample_rate!=target_sample_rate:\n",
        "  target_sample_rate = 16000\n",
        "  resampler = Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
        "  waveform = resampler(waveform)\n",
        "  sample_rate=target_sample_rate\n",
        "\n",
        "max_amplitude = torch.max(waveform)\n",
        "min_amplitude =torch.max(waveform)\n",
        "# Normalize the waveform between -1 and 1\n",
        "if max_amplitude<=1 and min_amplitude>=-1  :\n",
        "    max_amplitude1 = torch.max(torch.abs(waveform))\n",
        "    waveform = waveform / max_amplitude1\n",
        "\n",
        "mfcc_window_size=0.025\n",
        "mfcc_window_stride=0.01\n",
        "mfcc_n_mels=40\n",
        "fixed_length=500\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "mfcc = pad_or_trim_mfcc(mfcc)\n",
        "mfcc = mfcc.permute(0, 2, 1)\n",
        "outputs = model(mfcc.unsqueeze(0))\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "print(outputs.data)\n",
        "outputs1 = model1(mfcc.unsqueeze(0))\n",
        "_, predicted1 = torch.max(outputs1.data, 1)\n",
        "print(outputs1.data)\n",
        "label_map = {\"baankud'qaaa\": 0, \"bad'qa_aangura\": 1, \"anJa_chot'a_kalaa\": 2, \"baaluraghaat'a\": 3, \"baandhaakapi\": 4, \"anJa_bad'qa_kalaa\": 5, \"anJaanJa_haansa\": 6, \"anJaanJa_tailabiija\": 7, \"aapela\": 8, \"ad'qahara_d'aala\": 9,\"anJaanJa_aalu\": 10, \"anJaanJa_d'ima\": 11, \"aalu\": 12, \"aama\": 13, \"aanaarasa\": 14, \"aaii_aara_t'oyent'i\": 15, \"aalipuraduyaara\": 16, \"aalura_sanna_caala\": 17, \"aaii_aara_chatrisha\": 18, \"aaii_aara_kud'qi\": 19}\n",
        "print(\"Predicted class index:\",list(label_map.items())[predicted.item()])\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names1=os.listdir(data_dir)\n",
        "label_map1={class_name: idx for idx,class_name in enumerate(class_names1)}\n",
        "print(\"Predicted speaker index:\",list(label_map1.items())[predicted1.item()])\n",
        "import torch\n",
        "\n",
        "def calculate_class_percentage(outputs):\n",
        "    # Apply softmax to convert raw outputs to probabilities\n",
        "    probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    # Extract the probabilities for each class\n",
        "    class_probabilities = probabilities[0].tolist()  # Assuming outputs is of shape (1, num_classes)\n",
        "\n",
        "    return class_probabilities\n",
        "\n",
        "p1=calculate_class_percentage(outputs)\n",
        "p2=calculate_class_percentage(outputs1)\n",
        "print(p1)\n",
        "print(p2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6ATk8iFord-",
        "outputId": "dba1d314-2cba-494a-b087-71dd8b27d9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -9.2131,  -4.9201,  -4.1231,  -1.3675,  -5.2944,  -3.1048,  -2.7841,\n",
            "          -1.2166, -10.6023,  -2.6844,  -1.4464,  -3.4874, -12.1912, -10.0813,\n",
            "          -9.4554,  -8.2178,  -6.6966,  -6.2942,  -9.1473,  -9.3825]])\n",
            "tensor([[-10.2057,  -9.0569,  -2.2627,  -6.6084,  -7.0845,  -4.2889,  -5.4611,\n",
            "          -3.4328,  -5.5955, -11.1769,  -8.1311,   1.0151,   0.4642,  -1.1504,\n",
            "           1.9648,  -5.1953,  -4.2075,   8.8083,  -6.1464,   4.8413]])\n",
            "Predicted class index: ('anJaanJa_tailabiija', 7)\n",
            "Predicted speaker index: ('0041', 17)\n",
            "[9.737223444972187e-05, 0.007126555312424898, 0.015811603516340256, 0.2487250566482544, 0.00490104453638196, 0.04377659782767296, 0.060329731553792953, 0.28925129771232605, 2.427164690743666e-05, 0.06665221601724625, 0.2298642247915268, 0.029858840629458427, 4.955060376232723e-06, 4.086549233761616e-05, 7.642036507604644e-05, 0.0002634456614032388, 0.0012059294385835528, 0.0018033537780866027, 0.00010399498569313437, 8.220222662203014e-05]\n",
            "[5.412393644377289e-09, 1.7073663372002557e-08, 1.524039362266194e-05, 1.975547547772294e-07, 1.2271753746517788e-07, 2.0093111743335612e-06, 6.222294359758962e-07, 4.729649390355917e-06, 5.439711117105617e-07, 2.0493817753930443e-09, 4.309199752583481e-08, 0.00040413421811535954, 0.00023297018196899444, 4.635054210666567e-05, 0.0010446576634421945, 8.117089009829215e-07, 2.1796311102662003e-06, 0.9796992540359497, 3.1356364615930943e-07, 0.018545789644122124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "label_map = {\"baankud'qaaa\": 0, \"bad'qa_aangura\": 1, \"anJa_chot'a_kalaa\": 2, \"baaluraghaat'a\": 3, \"baandhaakapi\": 4, \"anJa_bad'qa_kalaa\": 5, \"anJaanJa_haansa\": 6, \"anJaanJa_tailabiija\": 7, \"aapela\": 8, \"ad'qahara_d'aala\": 9,\"anJaanJa_aalu\": 10, \"anJaanJa_d'ima\": 11, \"aalu\": 12, \"aama\": 13, \"aanaarasa\": 14, \"aaii_aara_t'oyent'i\": 15, \"aalipuraduyaara\": 16, \"aalura_sanna_caala\": 17, \"aaii_aara_chatrisha\": 18, \"aaii_aara_kud'qi\": 19}\n",
        "def plot_class_probabilities(class_probabilities, class_labels):\n",
        "    # Plotting the class probabilities\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.barh(list(class_labels.keys()), class_probabilities, color='skyblue')\n",
        "    ax.set_xlabel('Probability')\n",
        "    ax.set_ylabel('Class')\n",
        "    ax.set_title('Class Probabilities')\n",
        "    plt.show()\n",
        "plot_class_probabilities(p1, label_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "snp71cBbeSjb",
        "outputId": "bc635024-22f8-4c76-e13d-acaea5f32446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names1=os.listdir(data_dir)\n",
        "label_map1={class_name: idx for idx,class_name in enumerate(class_names1)}\n",
        "def plot_class_probabilities(class_probabilities, class_labels):\n",
        "    # Plotting the class probabilities\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.barh(list(class_labels.keys()), class_probabilities, color='skyblue')\n",
        "    ax.set_xlabel('Probability')\n",
        "    ax.set_ylabel('Class')\n",
        "    ax.set_title('Class Probabilities')\n",
        "    plt.show()\n",
        "plot_class_probabilities(p2, label_map1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "CpDE0QDvKw2F",
        "outputId": "8a7e745d-2bf9-405a-d186-ad91ba3be13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names=os.listdir(data_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "# Define the dataset class\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {class_name: idx for idx, class_name in enumerate(label_map)}\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg + torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_dataset'\n",
        "    class_names = os.listdir(data_dir)\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Load the saved model\n",
        "    model_path = '/content/drive/MyDrive/keyword_spotting_model_6.pth'\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model for additional epochs\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the updated model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/keyword_spotting_model_6_updated_1.pth')\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_dataset'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_spotting_model_6_updated_1.pth', validation_dataloader, device)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpdojrLlnii-",
        "outputId": "93d2c840-dfae-43c5-ccda-df8d1009cf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.924781606197357\n",
            "Epoch [2/10], Loss: 0.6383559173345565\n",
            "Epoch [3/10], Loss: 0.24158037588000297\n",
            "Epoch [4/10], Loss: 0.1288180309534073\n",
            "Epoch [5/10], Loss: 0.09747257977724075\n",
            "Epoch [6/10], Loss: 0.05562030494213104\n",
            "Epoch [7/10], Loss: 0.026374758817255498\n",
            "Epoch [8/10], Loss: 0.016677936483174564\n",
            "Epoch [9/10], Loss: 0.011654167482629419\n",
            "Epoch [10/10], Loss: 0.009184601455926896\n",
            "Validation Accuracy of the saved model: 84.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQBr8swSSH1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the TCResNet model\n",
        "data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "class_names=os.listdir(data_dir)\n",
        "label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg+torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Rest of the code remains unchanged...\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_Speakers_dataset'\n",
        "    class_names=os.listdir(data_dir)\n",
        "    label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/speaker_spotting_model.pth')\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    # validation_data_dir = '/content/drive/MyDrive/testing_Speakers_dataset'\n",
        "    # validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    # validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # # Test the saved model with the validation dataset\n",
        "    # test_saved_model('/content/drive/MyDrive/speaker_spotting_model.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f61d64-7adb-4553-860a-ff2954098d58",
        "id": "BqkLfC8USTzr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0118': 0, '0142': 1, '0149': 2, '0122': 3, '0143': 4, '0147': 5, '0127': 6, '0146': 7, '0135': 8, '0108': 9, '0073': 10, '0106': 11, '0100': 12, '0014': 13, '0009': 14, '0102': 15, '0099': 16, '0041': 17, '0084': 18, '0006': 19}\n",
            "Epoch [1/50], Loss: 2.9495139932632446\n",
            "Epoch [2/50], Loss: 2.7924093627929687\n",
            "Epoch [3/50], Loss: 2.6539785432815552\n",
            "Epoch [4/50], Loss: 2.527123007774353\n",
            "Epoch [5/50], Loss: 2.370954647064209\n",
            "Epoch [6/50], Loss: 2.271669263839722\n",
            "Epoch [7/50], Loss: 2.1229736375808717\n",
            "Epoch [8/50], Loss: 2.0101923084259035\n",
            "Epoch [9/50], Loss: 1.8908518409729005\n",
            "Epoch [10/50], Loss: 1.7904393458366394\n",
            "Epoch [11/50], Loss: 1.618246672153473\n",
            "Epoch [12/50], Loss: 1.4680297923088075\n",
            "Epoch [13/50], Loss: 1.320442681312561\n",
            "Epoch [14/50], Loss: 1.194735586643219\n",
            "Epoch [15/50], Loss: 1.1210867166519165\n",
            "Epoch [16/50], Loss: 0.9539926409721374\n",
            "Epoch [17/50], Loss: 0.8170857137441635\n",
            "Epoch [18/50], Loss: 0.7231274038553238\n",
            "Epoch [19/50], Loss: 0.5983017748594284\n",
            "Epoch [20/50], Loss: 0.4860734388232231\n",
            "Epoch [21/50], Loss: 0.42539854407310485\n",
            "Epoch [22/50], Loss: 0.3161561819911003\n",
            "Epoch [23/50], Loss: 0.2775924426317215\n",
            "Epoch [24/50], Loss: 0.23934102892875672\n",
            "Epoch [25/50], Loss: 0.15746699184179305\n",
            "Epoch [26/50], Loss: 0.11243901960551739\n",
            "Epoch [27/50], Loss: 0.06463075842708349\n",
            "Epoch [28/50], Loss: 0.03897902812808752\n",
            "Epoch [29/50], Loss: 0.025141068380326034\n",
            "Epoch [30/50], Loss: 0.02225900862365961\n",
            "Epoch [31/50], Loss: 0.018394415900111198\n",
            "Epoch [32/50], Loss: 0.015312229339033366\n",
            "Epoch [33/50], Loss: 0.012406188822351396\n",
            "Epoch [34/50], Loss: 0.008129575913771987\n",
            "Epoch [35/50], Loss: 0.006995051638223231\n",
            "Epoch [36/50], Loss: 0.005814961106516421\n",
            "Epoch [37/50], Loss: 0.005487407795153558\n",
            "Epoch [38/50], Loss: 0.005235326122492552\n",
            "Epoch [39/50], Loss: 0.004348044595681131\n",
            "Epoch [40/50], Loss: 0.004371806932613254\n",
            "Epoch [41/50], Loss: 0.003870985368266702\n",
            "Epoch [42/50], Loss: 0.0036234019370749593\n",
            "Epoch [43/50], Loss: 0.00345373566262424\n",
            "Epoch [44/50], Loss: 0.003014279536437243\n",
            "Epoch [45/50], Loss: 0.002736136286985129\n",
            "Epoch [46/50], Loss: 0.0026819168333895504\n",
            "Epoch [47/50], Loss: 0.0024409153242595494\n",
            "Epoch [48/50], Loss: 0.0022565128724090756\n",
            "Epoch [49/50], Loss: 0.0020160628436133267\n",
            "Epoch [50/50], Loss: 0.002249863762408495\n",
            "Validation Accuracy of the saved model: 71.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QQTPUAs1l4RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg+torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Rest of the code remains unchanged...\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = 21\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_keywords'\n",
        "    label_map = {}\n",
        "\n",
        "    for i in range(21):\n",
        "      key = f\"{i:03d}\"\n",
        "      label_map[key] = i\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/keyword_spotting_with_own_dataset_model_1_unknown.pth')\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_keywords'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_spotting_with_own_dataset_model_1_unknown.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f45af81-2840-4d44-e57d-4a05c3405863",
        "id": "tj2oG0FMl4qn"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '009': 9, '010': 10, '011': 11, '012': 12, '013': 13, '014': 14, '015': 15, '016': 16, '017': 17, '018': 18, '019': 19, '020': 20}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 2.8701979247006504\n",
            "Epoch [2/50], Loss: 2.5073325634002686\n",
            "Epoch [3/50], Loss: 2.230002533305775\n",
            "Epoch [4/50], Loss: 1.9740379615263506\n",
            "Epoch [5/50], Loss: 1.696793176911094\n",
            "Epoch [6/50], Loss: 1.4588215784593062\n",
            "Epoch [7/50], Loss: 1.263572627847845\n",
            "Epoch [8/50], Loss: 1.0549371242523193\n",
            "Epoch [9/50], Loss: 0.8651745644482699\n",
            "Epoch [10/50], Loss: 0.7356133948672902\n",
            "Epoch [11/50], Loss: 0.614414328878576\n",
            "Epoch [12/50], Loss: 0.5083629786968231\n",
            "Epoch [13/50], Loss: 0.4188829389485446\n",
            "Epoch [14/50], Loss: 0.365045501427217\n",
            "Epoch [15/50], Loss: 0.334569280797785\n",
            "Epoch [16/50], Loss: 0.2754190509969538\n",
            "Epoch [17/50], Loss: 0.20581930333917792\n",
            "Epoch [18/50], Loss: 0.16845078359950671\n",
            "Epoch [19/50], Loss: 0.14178822934627533\n",
            "Epoch [20/50], Loss: 0.10441598600961945\n",
            "Epoch [21/50], Loss: 0.08263196999376471\n",
            "Epoch [22/50], Loss: 0.08399161662567746\n",
            "Epoch [23/50], Loss: 0.07688694582744078\n",
            "Epoch [24/50], Loss: 0.07270827415314587\n",
            "Epoch [25/50], Loss: 0.060992390594699165\n",
            "Epoch [26/50], Loss: 0.04591987519101663\n",
            "Epoch [27/50], Loss: 0.03250043297355825\n",
            "Epoch [28/50], Loss: 0.02393443543802608\n",
            "Epoch [29/50], Loss: 0.019952664829113266\n",
            "Epoch [30/50], Loss: 0.015222986990755255\n",
            "Epoch [31/50], Loss: 0.013192426921291784\n",
            "Epoch [32/50], Loss: 0.013443598185073246\n",
            "Epoch [33/50], Loss: 0.011859174944799055\n",
            "Epoch [34/50], Loss: 0.0117968436838551\n",
            "Epoch [35/50], Loss: 0.011202774349261414\n",
            "Epoch [36/50], Loss: 0.009671267968687143\n",
            "Epoch [37/50], Loss: 0.008226114663888107\n",
            "Epoch [38/50], Loss: 0.0085687080652199\n",
            "Epoch [39/50], Loss: 0.006757220719009638\n",
            "Epoch [40/50], Loss: 0.006526884284209122\n",
            "Epoch [41/50], Loss: 0.006538378667425026\n",
            "Epoch [42/50], Loss: 0.00602940550412644\n",
            "Epoch [43/50], Loss: 0.006740854121744633\n",
            "Epoch [44/50], Loss: 0.005665798375213688\n",
            "Epoch [45/50], Loss: 0.005340100583535704\n",
            "Epoch [46/50], Loss: 0.004985386675054377\n",
            "Epoch [47/50], Loss: 0.004293980267406864\n",
            "Epoch [48/50], Loss: 0.0046609739815308285\n",
            "Epoch [49/50], Loss: 0.003917771560901945\n",
            "Epoch [50/50], Loss: 0.0035647224126891656\n",
            "Validation Accuracy of the saved model: 83.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    data_dir = '/content/drive/MyDrive/testing_keywords'\n",
        "    class_names=os.listdir(data_dir)\n",
        "    label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "    # Generate and print confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_keywords'\n",
        "    label_map = {}\n",
        "\n",
        "    for i in range(21):\n",
        "      key = f\"{i:03d}\"\n",
        "      label_map[key] = i\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_keywords'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset and generate confusion matrix\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_spotting_with_own_dataset_model_1_unknown.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub_neRikpnwA",
        "outputId": "d9294f5d-02fc-4356-b075-c5ff9a48c326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '009': 9, '010': 10, '011': 11, '012': 12, '013': 13, '014': 14, '015': 15, '016': 16, '017': 17, '018': 18, '019': 19, '020': 20}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy of the saved model: 83.72%\n",
            "Confusion Matrix:\n",
            "[[4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0]\n",
            " [0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 3 1 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 3 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg+torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Rest of the code remains unchanged...\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    data_dir = '/content/drive/MyDrive/testing_speakers'\n",
        "    class_names=os.listdir(data_dir)\n",
        "    label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_speakers'\n",
        "    label_map = {}\n",
        "    for i in range(21):\n",
        "      key = f\"{i:03d}\"\n",
        "      label_map[key] = i\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/keyword_speakers_with_own_dataset_model_1_unknown.pth')\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_speakers'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_speakers_with_own_dataset_model_1_unknown.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjzbb0ZKoBdi",
        "outputId": "c4d56555-e754-492b-e38b-a2b66dbb7ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '009': 9, '010': 10, '011': 11, '012': 12, '013': 13, '014': 14, '015': 15, '016': 16, '017': 17, '018': 18, '019': 19, '020': 20}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Loss: 2.6277717677029697\n",
            "Epoch [2/50], Loss: 1.867983796379783\n",
            "Epoch [3/50], Loss: 1.3932546702298252\n",
            "Epoch [4/50], Loss: 1.0552752343091099\n",
            "Epoch [5/50], Loss: 0.7790968309749257\n",
            "Epoch [6/50], Loss: 0.581100192936984\n",
            "Epoch [7/50], Loss: 0.4047762453556061\n",
            "Epoch [8/50], Loss: 0.2758385931903666\n",
            "Epoch [9/50], Loss: 0.25554009865630756\n",
            "Epoch [10/50], Loss: 0.18784923716024918\n",
            "Epoch [11/50], Loss: 0.1581945229660381\n",
            "Epoch [12/50], Loss: 0.11154821447350761\n",
            "Epoch [13/50], Loss: 0.08285175568678162\n",
            "Epoch [14/50], Loss: 0.07254724949598312\n",
            "Epoch [15/50], Loss: 0.06336434964429248\n",
            "Epoch [16/50], Loss: 0.08389557085253975\n",
            "Epoch [17/50], Loss: 0.08604972504756668\n",
            "Epoch [18/50], Loss: 0.06858701970089566\n",
            "Epoch [19/50], Loss: 0.050923278385942634\n",
            "Epoch [20/50], Loss: 0.0586988590657711\n",
            "Epoch [21/50], Loss: 0.03737586008554155\n",
            "Epoch [22/50], Loss: 0.01770667333833196\n",
            "Epoch [23/50], Loss: 0.012606319421055641\n",
            "Epoch [24/50], Loss: 0.009218261111527681\n",
            "Epoch [25/50], Loss: 0.00884873407300223\n",
            "Epoch [26/50], Loss: 0.00816040960225192\n",
            "Epoch [27/50], Loss: 0.007254610684784976\n",
            "Epoch [28/50], Loss: 0.005436581326648593\n",
            "Epoch [29/50], Loss: 0.005320912904360078\n",
            "Epoch [30/50], Loss: 0.005935186858881603\n",
            "Epoch [31/50], Loss: 0.003988971891389651\n",
            "Epoch [32/50], Loss: 0.003951337315480818\n",
            "Epoch [33/50], Loss: 0.00365821115503257\n",
            "Epoch [34/50], Loss: 0.003292942874726247\n",
            "Epoch [35/50], Loss: 0.003172333616848019\n",
            "Epoch [36/50], Loss: 0.0029618013967675242\n",
            "Epoch [37/50], Loss: 0.0030047914796424184\n",
            "Epoch [38/50], Loss: 0.003018308388577266\n",
            "Epoch [39/50], Loss: 0.002888815054161982\n",
            "Epoch [40/50], Loss: 0.002605515384030613\n",
            "Epoch [41/50], Loss: 0.002384891807609661\n",
            "Epoch [42/50], Loss: 0.0025935383746400476\n",
            "Epoch [43/50], Loss: 0.0021572463976388626\n",
            "Epoch [44/50], Loss: 0.0023913045667789197\n",
            "Epoch [45/50], Loss: 0.0023318669356575065\n",
            "Epoch [46/50], Loss: 0.0032715263501317663\n",
            "Epoch [47/50], Loss: 0.002787282626906579\n",
            "Epoch [48/50], Loss: 0.002543822364796969\n",
            "Epoch [49/50], Loss: 0.0022839063842018895\n",
            "Epoch [50/50], Loss: 0.0018535161242735658\n",
            "{'020': 0, '019': 1, '018': 2, '015': 3, '017': 4, '016': 5, '010': 6, '009': 7, '011': 8, '014': 9, '008': 10, '013': 11, '012': 12, '006': 13, '005': 14, '007': 15, '003': 16, '001': 17, '000': 18, '002': 19, '004': 20}\n",
            "Validation Accuracy of the saved model: 97.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    data_dir = '/content/drive/MyDrive/testing_speakers'\n",
        "    class_names=os.listdir(data_dir)\n",
        "    label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "    # Generate and print confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    label_map = {}\n",
        "\n",
        "    for i in range(21):\n",
        "      key = f\"{i:03d}\"\n",
        "      label_map[key] = i\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_speakers'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset and generate confusion matrix\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_speakers_with_own_dataset_model_1_unknown.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyNIIzevw7YS",
        "outputId": "416aef22-f836-4ca2-9742-f90cf4c97468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '009': 9, '010': 10, '011': 11, '012': 12, '013': 13, '014': 14, '015': 15, '016': 16, '017': 17, '018': 18, '019': 19, '020': 20}\n",
            "Validation Accuracy of the saved model: 97.62%\n",
            "Confusion Matrix:\n",
            "[[4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mC-cazRtKU_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torchvision.transforms import Compose\n",
        "import os\n",
        "import torch.nn as nn  # Import torch.nn module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import librosa\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "# Load the saved model\n",
        "bins = 40\n",
        "channels = [64, 128, 256, 512]\n",
        "channel_scale = 1\n",
        "num_classes = 21\n",
        "\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/keyword_spotting_with_own_dataset_model_1_unknown.pth'\n",
        "model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "model_path1 = '/content/drive/MyDrive/keyword_speakers_with_own_dataset_model_1_unknown.pth'\n",
        "model1 = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes)\n",
        "model1.load_state_dict(torch.load(model_path1, map_location=torch.device('cpu')))\n",
        "model1.eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pad_or_trim_mfcc(mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length >fixed_length:\n",
        "            mfcc = mfcc[:, :, :fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "dir_path=\"/content/drive/MyDrive/training_keywords/000\"\n",
        "key_thers=[]\n",
        "spe_thers=[]\n",
        "# Example usage\n",
        "for f in os.listdir(dir_path):\n",
        "      audio_file_path = os.path.join(dir_path,f)\n",
        "      waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "      if waveform.size(0)!=1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "\n",
        "\n",
        "      target_sample_rate = 16000\n",
        "      if sample_rate!=target_sample_rate:\n",
        "        target_sample_rate = 16000\n",
        "        resampler = Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "        sample_rate=target_sample_rate\n",
        "\n",
        "      max_amplitude = torch.max(waveform)\n",
        "      min_amplitude =torch.max(waveform)\n",
        "      # Normalize the waveform between -1 and 1\n",
        "      if max_amplitude<=1 and min_amplitude>=-1  :\n",
        "          max_amplitude1 = torch.max(torch.abs(waveform))\n",
        "          waveform = waveform / max_amplitude1\n",
        "\n",
        "      mfcc_window_size=0.025\n",
        "      mfcc_window_stride=0.01\n",
        "      mfcc_n_mels=40\n",
        "      fixed_length=500\n",
        "      mfcc = torchaudio.transforms.MFCC(\n",
        "                  sample_rate=sample_rate,\n",
        "                  n_mfcc=mfcc_n_mels,\n",
        "                  melkwargs={\n",
        "                      'win_length': int(mfcc_window_size * sample_rate),\n",
        "                      'hop_length': int(mfcc_window_stride * sample_rate),\n",
        "                  }\n",
        "              )(waveform)\n",
        "      mfcc = pad_or_trim_mfcc(mfcc)\n",
        "      mfcc = mfcc.permute(0, 2, 1)\n",
        "      outputs = model(mfcc.unsqueeze(0))\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      outputs1 = model1(mfcc.unsqueeze(0))\n",
        "      _, predicted1 = torch.max(outputs1.data, 1)\n",
        "      label_map = {}\n",
        "      for i in range(21):\n",
        "          key = f\"{i:03d}\"\n",
        "          label_map[key] = i\n",
        "      print(f\"{f},Predicted class index:\",list(label_map.items())[predicted.item()])\n",
        "      label_map1 = {}\n",
        "      for i in range(21):\n",
        "          key = f\"{i:03d}\"\n",
        "          label_map1[key] = i\n",
        "      print(f\"{f},Predicted speaker index:\",list(label_map1.items())[predicted1.item()])\n",
        "      p1=calculate_class_percentage(outputs)\n",
        "      p2=calculate_class_percentage(outputs1)\n",
        "      key_thers.append(p1)\n",
        "      spe_thers.append(p2)\n",
        "\n",
        "import torch\n",
        "\n",
        "def calculate_class_percentage(outputs):\n",
        "    # Apply softmax to convert raw outputs to probabilities\n",
        "    probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    # Extract the probabilities for each class\n",
        "    class_probabilities = probabilities[0].tolist()  # Assuming outputs is of shape (1, num_classes)\n",
        "\n",
        "    return class_probabilities\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e97ffe3-6807-473c-d8fc-a8289d598239",
        "id": "QJFKfNxlKZAl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MBNWB03B0137I030.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0137I030.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0065I030.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0065I030.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0099I030.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0099I030.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0095I030.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0095I030.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB05B0009I028.wav,Predicted class index: ('000', 0)\n",
            "MBNWB05B0009I028.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0065I027.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0065I027.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0135I027.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0135I027.wav,Predicted speaker index: ('018', 18)\n",
            "MBNWB03B0099I027.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0099I027.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB05B0009I027.wav,Predicted class index: ('000', 0)\n",
            "MBNWB05B0009I027.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0106I030.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0106I030.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0111I030.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0111I030.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0106I027.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0106I027.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0137I027.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0137I027.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0095I027.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0095I027.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0111I027.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0111I027.wav,Predicted speaker index: ('000', 0)\n",
            "MBNWB03B0135I030.wav,Predicted class index: ('000', 0)\n",
            "MBNWB03B0135I030.wav,Predicted speaker index: ('000', 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvSnKqU9wxw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QCiDvI1jCzMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "p1=spe_thers[6]\n",
        "label_map = {}\n",
        "for i in range(21):\n",
        "          key = f\"{i:03d}\"\n",
        "          label_map[key] = i\n",
        "def plot_class_probabilities(class_probabilities, class_labels):\n",
        "    # Plotting the class probabilities\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.barh(list(class_labels.keys()), class_probabilities, color='skyblue')\n",
        "    ax.set_xlabel('Probability')\n",
        "    ax.set_ylabel('Class')\n",
        "    ax.set_title('Class Probabilities')\n",
        "    plt.show()\n",
        "plot_class_probabilities(p1, label_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "f1518455-0d47-48bf-bc64-a46ca0151f4e",
        "id": "-gbTdKIiCpnQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mzDFfjNGVcmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "@Time : 2022/2/7 下午10:22\n",
        "@Author : Yang \"Jan\" Xiao\n",
        "@Description : keyword spotting model TC-ResNet.\n",
        "Reference by https://github.com/Doyosae/Temporal-Convolution-Resnet\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    data_dir = '/content/drive/MyDrive/testing_keywords'\n",
        "    class_names=os.listdir(data_dir)\n",
        "    label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "    # Generate and print confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_keywords'\n",
        "    label_map = {}\n",
        "\n",
        "    for i in range(21):\n",
        "      key = f\"{i:03d}\"\n",
        "      label_map[key] = i\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_keywords'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset and generate confusion matrix\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_spotting_with_own_dataset_model_1_unknown.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "fc2f5cb1-8d44-4cce-82cb-2b966d800ae4",
        "id": "UjepEAHIGoOg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '009': 9, '010': 10, '011': 11, '012': 12, '013': 13, '014': 14, '015': 15, '016': 16, '017': 17, '018': 18, '019': 19, '020': 20}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c0dccfb3b4ba>\u001b[0m in \u001b[0;36m<cell line: 190>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-c0dccfb3b4ba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Test the saved model with the validation dataset and generate confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mtest_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/keyword_spotting_with_own_dataset_model_1_unknown.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c0dccfb3b4ba>\u001b[0m in \u001b[0;36mtest_saved_model\u001b[0;34m(model_path, validation_dataloader, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mchannel_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTCResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchannel_scale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1149\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZCBdRNuA2PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KWS\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg+torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = 20\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "class KeywordSpottingDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_keywords_without_000'\n",
        "    label_map = {}\n",
        "\n",
        "    for i in range(20):\n",
        "      key = f\"{i+1:03d}\"\n",
        "      label_map[key] = i\n",
        "    print(label_map)\n",
        "    bins = 40 # frequency bands\n",
        "    channels = [64, 128, 256, 512]  #output feature maps\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = KeywordSpottingDataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/keyword_spotting_with_own_dataset_model_1_without_000.pth')\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_keywords_without_000'\n",
        "    validation_dataset = KeywordSpottingDataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_spotting_with_own_dataset_model_1_without_000.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2419dc8e-07f3-4cc9-e82e-e6ce372cfc01",
        "id": "LIFX2z8SHHyC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'001': 0, '002': 1, '003': 2, '004': 3, '005': 4, '006': 5, '007': 6, '008': 7, '009': 8, '010': 9, '011': 10, '012': 11, '013': 12, '014': 13, '015': 14, '016': 15, '017': 16, '018': 17, '019': 18, '020': 19}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 2.9342483997344972\n",
            "Epoch [2/50], Loss: 2.592533564567566\n",
            "Epoch [3/50], Loss: 2.2977112770080566\n",
            "Epoch [4/50], Loss: 1.9962242126464844\n",
            "Epoch [5/50], Loss: 1.7233738541603087\n",
            "Epoch [6/50], Loss: 1.5056581854820252\n",
            "Epoch [7/50], Loss: 1.207076358795166\n",
            "Epoch [8/50], Loss: 1.0087514817714691\n",
            "Epoch [9/50], Loss: 0.8325359284877777\n",
            "Epoch [10/50], Loss: 0.7686515033245087\n",
            "Epoch [11/50], Loss: 0.6094645440578461\n",
            "Epoch [12/50], Loss: 0.4858129233121872\n",
            "Epoch [13/50], Loss: 0.3889848619699478\n",
            "Epoch [14/50], Loss: 0.3022897019982338\n",
            "Epoch [15/50], Loss: 0.26176210343837736\n",
            "Epoch [16/50], Loss: 0.2467055931687355\n",
            "Epoch [17/50], Loss: 0.1811039611697197\n",
            "Epoch [18/50], Loss: 0.13822066634893418\n",
            "Epoch [19/50], Loss: 0.10742488279938697\n",
            "Epoch [20/50], Loss: 0.10312370471656322\n",
            "Epoch [21/50], Loss: 0.08333307579159736\n",
            "Epoch [22/50], Loss: 0.05808422788977623\n",
            "Epoch [23/50], Loss: 0.044850390404462814\n",
            "Epoch [24/50], Loss: 0.03935181461274624\n",
            "Epoch [25/50], Loss: 0.029308479651808737\n",
            "Epoch [26/50], Loss: 0.02409415002912283\n",
            "Epoch [27/50], Loss: 0.02326054163277149\n",
            "Epoch [28/50], Loss: 0.018437836226075886\n",
            "Epoch [29/50], Loss: 0.015598644129931926\n",
            "Epoch [30/50], Loss: 0.013142480421811343\n",
            "Epoch [31/50], Loss: 0.012538020964711905\n",
            "Epoch [32/50], Loss: 0.011353145912289619\n",
            "Epoch [33/50], Loss: 0.010287756007164717\n",
            "Epoch [34/50], Loss: 0.010081734461709856\n",
            "Epoch [35/50], Loss: 0.009545427840203046\n",
            "Epoch [36/50], Loss: 0.008396016666665673\n",
            "Epoch [37/50], Loss: 0.0077522510662674906\n",
            "Epoch [38/50], Loss: 0.0067613163962960245\n",
            "Epoch [39/50], Loss: 0.006782140675932169\n",
            "Epoch [40/50], Loss: 0.007112141000106931\n",
            "Epoch [41/50], Loss: 0.006211254419758916\n",
            "Epoch [42/50], Loss: 0.0059964384883642195\n",
            "Epoch [43/50], Loss: 0.005468942644074559\n",
            "Epoch [44/50], Loss: 0.005291244527325034\n",
            "Epoch [45/50], Loss: 0.005330358608625829\n",
            "Epoch [46/50], Loss: 0.0045256683137267825\n",
            "Epoch [47/50], Loss: 0.004873938206583261\n",
            "Epoch [48/50], Loss: 0.004339679679833353\n",
            "Epoch [49/50], Loss: 0.004405594523996114\n",
            "Epoch [50/50], Loss: 0.004409315553493798\n",
            "Validation Accuracy of the saved model: 72.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Speaker Identifiction\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the TCResNet model\n",
        "class TCResNet(nn.Module):\n",
        "    def __init__(self, bins, channels, num_classes):\n",
        "        super(TCResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=(bins, 3), padding=(0, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])  # Batch normalization added\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=(0, 1))\n",
        "        self.layer1 = self._make_layer(channels[0], channels[1], 2)\n",
        "        self.layer2 = self._make_layer(channels[1], channels[2], 2)\n",
        "        self.layer3 = self._make_layer(channels[2], channels[3], 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=(0, 1)))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))  # Batch normalization added\n",
        "            layers.append(nn.ReLU())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)  # Batch normalization added\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, device, num_epochs=10, weight_decay=0):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute L2 regularization\n",
        "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg+torch.norm(param)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
        "\n",
        "# Define the evaluation loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the function to test the saved model with the validation dataset\n",
        "def test_saved_model(model_path, validation_dataloader, device):\n",
        "    # Load the saved model\n",
        "    data_dir = '/content/drive/MyDrive/testing_speakers_without_000'\n",
        "    class_names=os.listdir(data_dir)\n",
        "    label_map={class_name: idx for idx,class_name in enumerate(class_names)}\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy of the saved model: {accuracy:.2f}%')\n",
        "\n",
        "class SP_ID_Dataset(Dataset):\n",
        "    def __init__(self, data_dir, label_map, mfcc_window_size=0.025, mfcc_window_stride=0.01, mfcc_n_mels=40, fixed_length=500):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_map = {str(k): v for k, v in label_map.items()}  # Convert keys to strings\n",
        "        self.mfcc_window_size = mfcc_window_size\n",
        "        self.mfcc_window_stride = mfcc_window_stride\n",
        "        self.mfcc_n_mels = mfcc_n_mels\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "        self.samples = []\n",
        "        for label_str, label in self.label_map.items():\n",
        "            label_dir = os.path.join(self.data_dir, label_str)\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith('.wav'):\n",
        "                    self.samples.append((os.path.join(label_dir, filename), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, label = self.samples[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.mfcc_n_mels,\n",
        "            melkwargs={\n",
        "                'win_length': int(self.mfcc_window_size * sample_rate),\n",
        "                'hop_length': int(self.mfcc_window_stride * sample_rate),\n",
        "            }\n",
        "        )(waveform)\n",
        "\n",
        "        # Pad or trim the MFCC tensor to fixed length\n",
        "        mfcc = self.pad_or_trim_mfcc(mfcc)\n",
        "\n",
        "        # Swap dimensions to match the expected shape by the model\n",
        "        mfcc = mfcc.permute(0, 2, 1)  # Swap the last two dimensions\n",
        "\n",
        "        return mfcc, label\n",
        "\n",
        "    def pad_or_trim_mfcc(self, mfcc):\n",
        "        # Get the current length of the MFCC tensor\n",
        "        current_length = mfcc.size(2)\n",
        "\n",
        "        # Pad or trim to fixed length\n",
        "        if current_length < self.fixed_length:\n",
        "            pad = torch.zeros(mfcc.size(0), mfcc.size(1), self.fixed_length - current_length)\n",
        "            mfcc = torch.cat((mfcc, pad), dim=2)\n",
        "        elif current_length > self.fixed_length:\n",
        "            mfcc = mfcc[:, :, :self.fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "def main():\n",
        "    # Define the parameters\n",
        "    data_dir = '/content/drive/MyDrive/training_speakers_without_000'\n",
        "    label_map = {}\n",
        "    for i in range(20):\n",
        "      key = f\"{i+1:03d}\"\n",
        "      label_map[key] = i\n",
        "    print(label_map)\n",
        "    bins = 40\n",
        "    channels = [64, 128, 256, 512]\n",
        "    channel_scale = 1\n",
        "    num_classes = len(label_map)\n",
        "    batch_size = 32\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    fixed_length = 500\n",
        "\n",
        "    # Create the dataset and dataloader for training\n",
        "    dataset = SP_ID_Dataset(data_dir, label_map, fixed_length=fixed_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TCResNet(bins, [int(cha * channel_scale) for cha in channels], num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train(model, dataloader, criterion, optimizer, device, num_epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/keyword_speakers_with_own_dataset_model_1_without_unknown.pth')\n",
        "\n",
        "    # Create the dataset and dataloader for validation\n",
        "    validation_data_dir = '/content/drive/MyDrive/testing_speakers_without_000'\n",
        "    validation_dataset = SP_ID_Dataset(validation_data_dir, label_map, fixed_length=fixed_length)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test the saved model with the validation dataset\n",
        "    test_saved_model('/content/drive/MyDrive/keyword_speakers_with_own_dataset_model_1_without_unknown.pth', validation_dataloader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec261cd9-4b4a-4a54-c76e-5398975c7cc6",
        "id": "4uWm7yhpIJJg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'001': 0, '002': 1, '003': 2, '004': 3, '005': 4, '006': 5, '007': 6, '008': 7, '009': 8, '010': 9, '011': 10, '012': 11, '013': 12, '014': 13, '015': 14, '016': 15, '017': 16, '018': 17, '019': 18, '020': 19}\n",
            "Epoch [1/50], Loss: 2.571332597732544\n",
            "Epoch [2/50], Loss: 1.8002379775047301\n",
            "Epoch [3/50], Loss: 1.3154489398002625\n",
            "Epoch [4/50], Loss: 0.9577666223049164\n",
            "Epoch [5/50], Loss: 0.6951274067163468\n",
            "Epoch [6/50], Loss: 0.488301956653595\n",
            "Epoch [7/50], Loss: 0.40423802435398104\n",
            "Epoch [8/50], Loss: 0.32378932237625124\n",
            "Epoch [9/50], Loss: 0.2520218759775162\n",
            "Epoch [10/50], Loss: 0.20089017003774642\n",
            "Epoch [11/50], Loss: 0.17033570110797883\n",
            "Epoch [12/50], Loss: 0.155753480643034\n",
            "Epoch [13/50], Loss: 0.08630088567733765\n",
            "Epoch [14/50], Loss: 0.0643706776201725\n",
            "Epoch [15/50], Loss: 0.05059273298829794\n",
            "Epoch [16/50], Loss: 0.0660213928669691\n",
            "Epoch [17/50], Loss: 0.0438255600631237\n",
            "Epoch [18/50], Loss: 0.029765952192246915\n",
            "Epoch [19/50], Loss: 0.024378932360559703\n",
            "Epoch [20/50], Loss: 0.017792620602995156\n",
            "Epoch [21/50], Loss: 0.013417454436421394\n",
            "Epoch [22/50], Loss: 0.011145391222089528\n",
            "Epoch [23/50], Loss: 0.008435173239558936\n",
            "Epoch [24/50], Loss: 0.007704467419534922\n",
            "Epoch [25/50], Loss: 0.006889217486605048\n",
            "Epoch [26/50], Loss: 0.007815197249874473\n",
            "Epoch [27/50], Loss: 0.005241993023082614\n",
            "Epoch [28/50], Loss: 0.005579483858309686\n",
            "Epoch [29/50], Loss: 0.005617547361180186\n",
            "Epoch [30/50], Loss: 0.004952127602882684\n",
            "Epoch [31/50], Loss: 0.004952103015966713\n",
            "Epoch [32/50], Loss: 0.003748364746570587\n",
            "Epoch [33/50], Loss: 0.003463055286556482\n",
            "Epoch [34/50], Loss: 0.0032457804307341577\n",
            "Epoch [35/50], Loss: 0.003363049332983792\n",
            "Epoch [36/50], Loss: 0.0034061735961586235\n",
            "Epoch [37/50], Loss: 0.0028014226583763955\n",
            "Epoch [38/50], Loss: 0.003028581151738763\n",
            "Epoch [39/50], Loss: 0.00380858548451215\n",
            "Epoch [40/50], Loss: 0.0033538108109496534\n",
            "Epoch [41/50], Loss: 0.002813377580605447\n",
            "Epoch [42/50], Loss: 0.002600713784340769\n",
            "Epoch [43/50], Loss: 0.0027902247617021202\n",
            "Epoch [44/50], Loss: 0.0024242239072918893\n",
            "Epoch [45/50], Loss: 0.002331769221927971\n",
            "Epoch [46/50], Loss: 0.0024989124154672026\n",
            "Epoch [47/50], Loss: 0.002382095600478351\n",
            "Epoch [48/50], Loss: 0.0022295156377367675\n",
            "Epoch [49/50], Loss: 0.0019959187135100364\n",
            "Epoch [50/50], Loss: 0.0019329963251948357\n",
            "{'020': 0, '019': 1, '017': 2, '011': 3, '012': 4, '010': 5, '015': 6, '013': 7, '014': 8, '016': 9, '018': 10, '009': 11, '003': 12, '008': 13, '002': 14, '004': 15, '001': 16, '007': 17, '006': 18, '005': 19}\n",
            "Validation Accuracy of the saved model: 97.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0Ut2_LOEwvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}